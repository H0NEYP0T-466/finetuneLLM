# FineTuneLLM - Terminal Style Chat Interface

A minimalistic terminal-style chat UI with React+TypeScript frontend and FastAPI backend for local LLM inference.

## Features

- ğŸ–¥ï¸ Clean terminal-style chat interface with #111 background
- ğŸš€ FastAPI backend with local LLM support (GGUF format)
- ğŸ“Š Real-time token streaming
- ğŸ’¾ MongoDB chat history storage
- ğŸ¨ Rich and colorful server logs
- âš¡ Auto-loads last 20 messages on startup
- ğŸ“ **NEW: Fine-tune Phi-2 on custom datasets** ([Quick Start](COLAB_QUICKSTART.md) | [Full Guide](finetune.md))

## Prerequisites

- Node.js 18+
- Python 3.9+
- MongoDB (running locally on port 27017)

## Project Structure

```
finetuneLLM/
â”œâ”€â”€ src/                    # Frontend source
â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”œâ”€â”€ services/          # API services
â”‚   â”œâ”€â”€ types/             # TypeScript types
â”‚   â””â”€â”€ styles/            # CSS styles
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â””â”€â”€ main.py        # FastAPI application
â”‚   â”œâ”€â”€ model/             # Place .gguf model files here
â”‚   â””â”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md
```

## Setup Instructions

### Option 1: Using Docker Compose (Recommended)

This is the easiest way to run both the backend and MongoDB:

1. Place your GGUF model file in `backend/model/` directory

2. Start all services:
```bash
docker-compose up -d
```

3. Check logs:
```bash
docker-compose logs -f backend
```

4. Stop services:
```bash
docker-compose down
```

The backend will run on `http://localhost:8002` and MongoDB on `localhost:27017`.

**Note:** Using docker-compose ensures:
- âœ… MongoDB is properly connected (no connection refused errors)
- âœ… Database `finetuneLLM` is automatically created
- âœ… Messages are persisted across restarts
- âœ… Proper networking between services

### Option 2: Manual Setup

#### Backend Setup

1. Install Python dependencies:
```bash
cd backend
pip install -r requirements.txt
```

2. (Optional) Configure environment variables:
```bash
cd backend
cp .env.example .env
# Edit .env to customize MongoDB URI and CORS settings
```

3. Place your GGUF model file in `backend/model/` directory

4. Ensure MongoDB is running:
```bash
# On Linux/Mac
sudo systemctl start mongod

# Or with Docker
docker run -d -p 27017:27017 --name mongodb mongo
```

5. Start the backend server:
```bash
cd backend
./start_server.sh
# Or manually:
# cd app && python main.py
```

The backend will run on `http://localhost:8000`

### Frontend Setup

1. Install dependencies:
```bash
npm install
```

2. (Optional) Configure environment variables:
```bash
cp .env.example .env
# Edit .env to customize API URL if needed
```

3. Start the development server:
```bash
npm run dev
```

The frontend will run on `http://localhost:5173`

## Usage

1. Start MongoDB
2. Start the backend server (it will load the LLM model)
3. Start the frontend development server
4. Open your browser to `http://localhost:5173`
5. Start chatting!

## Features in Detail

### Backend
- Loads GGUF models on server startup
- Streams tokens in real-time using Server-Sent Events
- Logs user prompts and model responses with rich formatting
- Tracks and logs response times
- Stores all conversations in MongoDB

### Frontend
- Terminal-style UI with green text on black background (#111)
- Token-by-token streaming display
- Loading indicators during model initialization
- Auto-scroll to latest messages
- Loads last 20 messages from database on startup

## API Endpoints

- `GET /` - Health check
- `GET /status` - Check model and database status
- `GET /messages?limit=20` - Get last N messages
- `POST /chat` - Send message and stream response

## Development

Build for production:
```bash
npm run build
```

Preview production build:
```bash
npm run preview
```

## Fine-Tuning

Want to fine-tune the Phi-2 model on your own data? We've got you covered!

### ğŸš€ Quick Start

1. Prepare your dataset as `dataset.xlsx` (Excel file with Q&A pairs)
2. Upload to Google Colab and run `finetuneCollab.py`
3. Get your fine-tuned model in 20-30 minutes!

**Read the guides:**
- [Colab Quick Start](COLAB_QUICKSTART.md) - Get started in 5 minutes
- [Complete Guide](finetune.md) - Full technical documentation
- [Feature Overview](FINETUNE_README.md) - What's included

### What You Get

- âœ… Parameter-efficient fine-tuning with LoRA
- âœ… Runs on free Google Colab GPU
- âœ… Automatic training visualizations
- âœ… Complete documentation
- âœ… Example dataset included

### Files

- `finetuneCollab.py` - Main training script
- `finetune.md` - Technical documentation (25KB)
- `COLAB_QUICKSTART.md` - Quick start guide
- `requirements-finetune.txt` - Dependencies
- `dataset_example.xlsx` - Sample data

## License

MIT
